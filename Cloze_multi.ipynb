{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e266eca-b017-461f-9be4-bec02cae9b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fae8a7a9-6902-424e-8e66-b107fccb361f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fe3aa20bef0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "from dataclasses import dataclass\n",
    "import json\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from llamawrapper import load_unemb_only, LlamaHelper\n",
    "import seaborn as sns\n",
    "from scipy.stats import bootstrap\n",
    "from utils import plot_ci, plot_ci_plus_heatmap\n",
    "from tqdm import tqdm\n",
    "\n",
    "# fix random seed\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1fb93987",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "target_lang = 'zh'\n",
    "model_size = '13b'\n",
    "out_dir = './visuals'\n",
    "# Need a huggingface token for some model\n",
    "hf_token = ''\n",
    "\n",
    "latent_lang_1 = 'en'\n",
    "latent_lang_2 = 'fr'\n",
    "model_selected = 2\n",
    "layer_num = 40\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "430b2665",
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_selected == 1:\n",
    "    custom_model = 'llm-jp/llm-jp-13b-v2.0'\n",
    "elif model_selected == 2:\n",
    "    custom_model = 'meta-llama/Llama-2-%s-hf'%model_size\n",
    "elif model_selected == 3:\n",
    "    custom_model = 'tokyotech-llm/Swallow-%s-hf'%model_size\n",
    "elif model_selected == 4:\n",
    "    custom_model = 'hfl/chinese-llama-2-13b'\n",
    "elif model_selected == 5:\n",
    "    custom_model = 'baichuan-inc/Baichuan2-13B-Base'\n",
    "elif model_selected == 6:\n",
    "    custom_model = 'sambanovasystems/SambaLingo-Arabic-Base'\n",
    "elif model_selected == 7:\n",
    "    custom_model = 'OpenLLM-France/Claire-Mistral-7B-0.1'\n",
    "elif model_selected == 8:\n",
    "    custom_model = 'croissantllm/CroissantLLMBase'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1cca52f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4e8141b-1c21-4132-86d1-e537e6244bf0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhong/miniconda3/envs/latent/lib/python3.12/site-packages/transformers/models/auto/tokenization_auto.py:769: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "/home/zhong/miniconda3/envs/latent/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py:468: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d58030bb8374d41855f8a0baef780fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if custom_model is not None:\n",
    "    model = LlamaHelper(dir=custom_model, load_in_8bit=True, hf_token=hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f081bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = model.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d9fe283",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'multi5.csv'\n",
    "df = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "14f9e530",
   "metadata": {},
   "outputs": [],
   "source": [
    "def capitalizations(token_str):\n",
    "    variants = [token_str.lower(), token_str.capitalize()]\n",
    "    return list(set(variants))\n",
    "\n",
    "def add_spaces_(tokens):\n",
    "    return ['▁' + t for t in tokens] + tokens\n",
    "\n",
    "def add_spaces(tokens):\n",
    "    return [' ' + t for t in tokens] + tokens\n",
    "\n",
    "def add_(tokens):\n",
    "    return ['_' + t for t in tokens]\n",
    "\n",
    "def ar_(tokens):\n",
    "    return ['\"' + t for t in tokens]\n",
    "\n",
    "# Obtain all possible tokenizations of a word\n",
    "def process_tokens(token_str: str, tokenizer, lang):\n",
    "    final_tokens = []\n",
    "    # print(token_str)\n",
    "    if lang in ['en', 'fr']:\n",
    "        with_capitalizations = capitalizations(token_str)\n",
    "        if model_selected == 9:\n",
    "            with_spaces = add_spaces(with_capitalizations)\n",
    "        else:\n",
    "            with_spaces = add_spaces_(with_capitalizations)\n",
    "        finail = with_spaces + add_(with_capitalizations)\n",
    "        finail = list(set(finail))\n",
    "        for tok in finail:\n",
    "            default_tokens = tokenizer.tokenize(tok)\n",
    "            default_token_ids = tokenizer.convert_tokens_to_ids(default_tokens)\n",
    "\n",
    "            if tok.startswith('_') and len(default_token_ids) > 1:\n",
    "                if model_selected == 1:\n",
    "                    final_tokens.append(default_token_ids[2:])\n",
    "                else:\n",
    "                    final_tokens.append(default_token_ids[1:])\n",
    "            elif model_selected == 1 and lang == 'fr':\n",
    "                final_tokens.append(default_token_ids[:])\n",
    "                if default_token_ids[0] == 31:\n",
    "                    final_tokens.append(default_token_ids[1:])\n",
    "            else:\n",
    "                final_tokens.append(default_token_ids)\n",
    "    elif lang in ['ja', 'zh']:\n",
    "        finail = ['▁' + token_str] + [token_str]\n",
    "        for tok in finail:\n",
    "            default_tokens = tokenizer.tokenize(tok)\n",
    "            default_token_ids = tokenizer.convert_tokens_to_ids(default_tokens)\n",
    "            final_tokens.append(default_token_ids)\n",
    "            if len(default_token_ids) > 1 and model_selected!=5:\n",
    "                final_tokens.append(default_token_ids[1:])\n",
    "    elif lang in ['ar']:\n",
    "        finail = ['▁' + token_str] + [token_str] + ['\"' + token_str]\n",
    "        for tok in finail:\n",
    "            default_tokens = tokenizer.tokenize(tok)\n",
    "            default_token_ids = tokenizer.convert_tokens_to_ids(default_tokens)\n",
    "            if tok.startswith('\"'):\n",
    "                final_tokens.append(default_token_ids[1:])\n",
    "            else:\n",
    "                final_tokens.append(default_token_ids)\n",
    "                if len(default_token_ids) > 1:\n",
    "                    final_tokens.append(default_token_ids[1:])\n",
    "    return final_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b8be7ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicates(nested_list):\n",
    "    if len(nested_list) <= 1:\n",
    "        return nested_list\n",
    "    return [list(x) for x in set(tuple(x) for x in nested_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6d53abf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "166it [00:00, 776.78it/s]\n"
     ]
    }
   ],
   "source": [
    "key = f'{target_lang}_prompt_masked'\n",
    "# blank_prompt_translation_masked\n",
    "dataset_gap = []\n",
    "n_skip = 2\n",
    "for idx, (idx_df, row) in tqdm(enumerate(df.iterrows())):\n",
    "    prompt_template = f''\n",
    "    indices = set(list(range(len(df[key])))) - set([idx])\n",
    "    idx_examples = np.random.choice(list(indices), n_skip, replace=False)\n",
    "    prompt_template += f'{df[key][idx_examples[0]]}\\n'\n",
    "    prompt_template += f'{df[key][idx_examples[1]]}\\n' \n",
    "\n",
    "    out_token_str = row[target_lang]\n",
    "    out_token_id = process_tokens(out_token_str, tokenizer, target_lang)\n",
    "    out_token_str2 = row[target_lang + '1']\n",
    "    out_token_id2 = process_tokens(out_token_str2, tokenizer, target_lang)\n",
    "    out_token_id = out_token_id + out_token_id2\n",
    "    out_token_id = remove_duplicates(out_token_id)\n",
    "\n",
    "    latent_token_str_1 = row[latent_lang_1]\n",
    "    latent_token_id_1 = process_tokens(latent_token_str_1, tokenizer, latent_lang_1)\n",
    "    latent_token_str_2 = row[latent_lang_2]\n",
    "    latent_token_id_2 = process_tokens(latent_token_str_2, tokenizer, latent_lang_2)\n",
    "\n",
    "    latent_token_str_11 = row[latent_lang_1 + '1']\n",
    "    latent_token_id_11 = process_tokens(latent_token_str_11, tokenizer, latent_lang_1)\n",
    "    latent_token_str_22 = row[latent_lang_2 + '1']\n",
    "    latent_token_id_22 = process_tokens(latent_token_str_22, tokenizer, latent_lang_2)\n",
    "\n",
    "    latent_token_id_1 = latent_token_id_1 + latent_token_id_11\n",
    "    latent_token_id_2 = latent_token_id_2 + latent_token_id_22\n",
    "    latent_token_id_1 = remove_duplicates(latent_token_id_1)\n",
    "    latent_token_id_2 = remove_duplicates(latent_token_id_2)\n",
    "\n",
    "    prompt = row[key].split(\":\")[0]+\": \\\"\"\n",
    "    dataset_gap.append({\n",
    "        'prompt': prompt_template + prompt,\n",
    "        'out_token_id': out_token_id,\n",
    "        'out_token_str': out_token_str,\n",
    "        'latent_token_id_1': latent_token_id_1,\n",
    "        'latent_token_str_1': latent_token_str_1,\n",
    "        'latent_token_id_2': latent_token_id_2,\n",
    "        'latent_token_str_2': latent_token_str_2,\n",
    "\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f787125a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "166"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset_gap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3c8c342d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gap = pd.DataFrame(dataset_gap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bd671dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(f'{os.path.join(out_dir, custom_model)}/cloze', exist_ok=True)\n",
    "df_gap.to_csv(f'{os.path.join(out_dir, custom_model)}/cloze/{target_lang}_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "960af8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_sequence_probabilities(prompt, tokenizer, token_sequences, model, where, model_selected, layer_num):\n",
    "\n",
    "    prompt_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
    "    if model_selected == 1:\n",
    "        prompt_ids = prompt_ids[:, :-1]\n",
    "    total_probabilities = torch.zeros(layer_num)\n",
    "    for token_sequence in token_sequences:\n",
    "        sequence_probabilities = torch.ones(layer_num)\n",
    "        current_input_ids = prompt_ids\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for token_id in token_sequence:\n",
    "                outputs = model(current_input_ids, output_hidden_states=True)\n",
    "                layer_probabilities = torch.ones(layer_num)\n",
    "\n",
    "                for layer_id in range(layer_num):\n",
    "                    layer_output = model.model.layers[layer_id].output\n",
    "\n",
    "                    if where == 1:\n",
    "                        normed = model.model.norm(layer_output)\n",
    "                    elif where == 2:\n",
    "                        normed = model.model.layers[layer_id].post_attention_layernorm(layer_output)\n",
    "\n",
    "                    logits = model.lm_head(normed)\n",
    "                    token_probabilities = logits[:, -1, :].softmax(dim=-1).detach().cpu()[:, token_id]\n",
    "                    layer_probabilities[layer_id] = token_probabilities\n",
    "\n",
    "                sequence_probabilities *= layer_probabilities\n",
    "                new_input_ids = torch.tensor([[token_id]], device=current_input_ids.device)\n",
    "                current_input_ids = torch.cat([current_input_ids, new_input_ids], dim=1)\n",
    "        \n",
    "        total_probabilities += sequence_probabilities\n",
    "\n",
    "    return total_probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "55964c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(f'{os.path.join(out_dir, custom_model)}/cloze', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d485bfb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_gap_sub=dataset_gap[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_token_probs_1 = []\n",
    "latent_token_probs_2 = []\n",
    "out_token_probs = []\n",
    "equal = 0\n",
    "# in_token_probs = []\n",
    "\n",
    "end = -2 if model_selected == 1 else -1\n",
    "for idx, d in tqdm(enumerate(dataset_gap)):\n",
    "    out_probs = compute_sequence_probabilities(prompt = d['prompt'], tokenizer = tokenizer, token_sequences = d['out_token_id'], model = model.model, where = 1, model_selected = model_selected, layer_num = layer_num)\n",
    "    if target_lang == latent_lang_1:\n",
    "        equal = 1\n",
    "        latent_probs_1 = out_probs\n",
    "    else:\n",
    "        latent_probs_1 = compute_sequence_probabilities(prompt = d['prompt'], tokenizer = tokenizer, token_sequences = d['latent_token_id_1'], model = model.model, where = 1, model_selected = model_selected, layer_num = layer_num)\n",
    "    if target_lang == latent_lang_2:\n",
    "        equal = 2\n",
    "        latent_probs_2 = out_probs\n",
    "    elif latent_lang_1 == latent_lang_2:\n",
    "        latent_probs_2 = latent_probs_1\n",
    "    else:\n",
    "        latent_probs_2 = compute_sequence_probabilities(prompt = d['prompt'], tokenizer = tokenizer, token_sequences = d['latent_token_id_2'], model = model.model, where = 1, model_selected = model_selected, layer_num = layer_num)\n",
    "    # in_probs = compute_sequence_probabilities(prompt = d['prompt'], tokenizer = tokenizer, token_sequences = d['in_token_id'], llama = model, unemb = unemb, end = end)\n",
    "\n",
    "    latent_token_probs_1.append(latent_probs_1)\n",
    "    latent_token_probs_2.append(latent_probs_2)\n",
    "    out_token_probs.append(out_probs)\n",
    "    # in_token_probs.append(in_probs)\n",
    "\n",
    "latent_token_probs_1 = torch.stack(latent_token_probs_1)\n",
    "latent_token_probs_2 = torch.stack(latent_token_probs_2)\n",
    "out_token_probs = torch.stack(out_token_probs)\n",
    "# in_token_probs_11 = torch.stack(in_token_probs)\n",
    "np.savez(f'{os.path.join(out_dir, custom_model)}/cloze/{model_size}_{target_lang}_probas.npz',\n",
    "         latent_token_probs_1=latent_token_probs_1,\n",
    "         latent_token_probs_2=latent_token_probs_2,\n",
    "         out_token_probs=out_token_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7becd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_ci_mk2(ax, data, label, color='blue', linestyle='-', tik_step=4, layer_num = 40):\n",
    "\n",
    "    data = np.array(data)\n",
    "    for i in range(0, layer_num+1, tik_step):\n",
    "        ax.axvline(i, color='black', linestyle='--', alpha=0.2, linewidth=1)\n",
    "    mean = data.mean(axis=0)\n",
    "    std = data.std(axis=0)\n",
    "    std_err = std / np.sqrt(data.shape[0])\n",
    "\n",
    "    data_ci = {\n",
    "        'x': np.arange(data.shape[1]) + 1,\n",
    "        'y': mean,\n",
    "        'y_upper': mean + 1.96 * std_err,\n",
    "        'y_lower': mean - 1.96 * std_err,\n",
    "    }\n",
    "\n",
    "    df = pd.DataFrame(data_ci)\n",
    "    ax.plot(df['x'], df['y'], label=label, color=color, linestyle=linestyle)\n",
    "    ax.fill_between(df['x'], df['y_lower'], df['y_upper'], color=color, alpha=0.3)\n",
    "\n",
    "def plot_ci_plus_heatmap_mk2(latent_token_probs_1, latent_token_probs_2, out_token_probs, latent_lang_1, latent_lang_2, target_lang, color='blue', tik_step=4, equal = 0, layer_num = 40):\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(6, 4))\n",
    "    \n",
    "    if equal == 1:\n",
    "        plot_ci_mk2(ax, latent_token_probs_1, f'{latent_lang_1}_target', color='tab:orange', tik_step=tik_step, layer_num=layer_num)\n",
    "    else: \n",
    "        plot_ci_mk2(ax, latent_token_probs_1, f'{latent_lang_1}_latent', color='tab:orange', tik_step=tik_step, layer_num=layer_num)\n",
    "    if equal == 2:\n",
    "        plot_ci_mk2(ax, latent_token_probs_2, f'{latent_lang_2}_target', color='tab:red', tik_step=tik_step, layer_num=layer_num)\n",
    "    else:\n",
    "        plot_ci_mk2(ax, latent_token_probs_2, f'{latent_lang_2}_latent', color='tab:red', tik_step=tik_step, layer_num=layer_num)\n",
    "    \n",
    "    if equal == 0:\n",
    "        plot_ci_mk2(ax, out_token_probs, f'{target_lang}_target', color='tab:blue', tik_step=tik_step)\n",
    "    \n",
    "    ax.set_xlabel('layer')\n",
    "    ax.set_ylabel('probability')\n",
    "    \n",
    "    ax.set_xlim(0, layer_num)\n",
    "    ax.set_ylim(0, 1)\n",
    "    \n",
    "    ax.set_xticks(np.arange(0, layer_num+1, step=tik_step))\n",
    "    ax.set_yticks([0, 0.5, 1.0])\n",
    "    ax.legend(loc='upper left')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    return fig, ax\n",
    "\n",
    "\n",
    "fig, ax = plot_ci_plus_heatmap_mk2(latent_token_probs_1, latent_token_probs_2, out_token_probs, latent_lang_1, latent_lang_2, target_lang, color='tab:orange', tik_step=5, equal=equal, layer_num=layer_num)\n",
    "\n",
    "plt.savefig(f'{os.path.join(out_dir, custom_model)}/cloze/{model_size}_{target_lang}_probas_ent.pdf', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6fb5d1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def custom_beam_search(model, tokenizer, device, prompt, num_beams=5, max_length=50, layer_id=39):\n",
    "    prompt_ids = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
    "\n",
    "    all_candidates = []\n",
    "    all_probabilities = []\n",
    "\n",
    "    for step in range(max_length):\n",
    "        if not all_candidates:\n",
    "            with torch.no_grad():\n",
    "                outputs = model(prompt_ids)\n",
    "                layer_output = model.model.layers[layer_id].output\n",
    "                normed = model.model.norm(layer_output)\n",
    "                logits = model.lm_head(normed)\n",
    "                probs = F.softmax(logits[:, -1, :], dim=-1)\n",
    "                top_probs, top_indices = torch.topk(probs, num_beams, dim=1)\n",
    "            for i in range(num_beams):\n",
    "                all_candidates.append([top_indices[0, i].item()])\n",
    "                all_probabilities.append(top_probs[0, i].item())\n",
    "        else:\n",
    "            new_candidates = []\n",
    "            new_probabilities = []\n",
    "\n",
    "            for i in range(len(all_candidates)):\n",
    "                candidate_ids = prompt_ids.clone()\n",
    "                candidate_ids = torch.cat([candidate_ids.squeeze(0), torch.tensor(all_candidates[i], device=device)], dim=0).unsqueeze(0)\n",
    "                with torch.no_grad():\n",
    "                    outputs = model(candidate_ids)\n",
    "                    layer_output = model.model.layers[layer_id].output\n",
    "                    normed = model.model.norm(layer_output)\n",
    "                    logits = model.lm_head(normed)\n",
    "                    probs = F.softmax(logits[:, -1, :], dim=-1)\n",
    "                    top_probs, top_indices = torch.topk(probs, num_beams, dim=1)\n",
    "\n",
    "                for j in range(num_beams):\n",
    "                    new_candidates.append(all_candidates[i] + [top_indices[0, j].item()])\n",
    "                    new_probabilities.append(all_probabilities[i] * top_probs[0, j].item())\n",
    "\n",
    "            all_candidates = new_candidates\n",
    "            all_probabilities = new_probabilities\n",
    "\n",
    "\n",
    "            top_indices = torch.topk(torch.tensor(all_probabilities), num_beams, largest=True).indices\n",
    "            all_candidates = [all_candidates[i] for i in top_indices]\n",
    "            all_probabilities = [all_probabilities[i] for i in top_indices]\n",
    "\n",
    "    final_results = [tokenizer.decode(candidate) for candidate in all_candidates]\n",
    "    return list(zip(final_results, all_candidates, all_probabilities))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "938c657b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence: 银行, Ids: [3001], Probability: 0.1475\n",
      "Sequence: 金融机构, Ids: [19177], Probability: 0.0479\n",
      "Sequence: 银行的, Ids: [35347], Probability: 0.0367\n",
      "Sequence: 存款, Ids: [11243], Probability: 0.0314\n",
      "Sequence: 商业银行, Ids: [30453], Probability: 0.0273\n",
      "Sequence: 银行存款, Ids: [59412], Probability: 0.0223\n",
      "Sequence: 在银行, Ids: [82798], Probability: 0.0190\n",
      "Sequence: ATM, Ids: [71456], Probability: 0.0168\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'prompt': '\"__\"は、何かにかかるお金です。答え: \"費用\"\\n\"__\"は、何かをするためのチャンスです。答え: \"機会\"\\n\"__\"は、お金を預けたり借りたりする場所です。答え: \"',\n",
       " 'out_token_id': [[92311, 101578, 92406],\n",
       "  [3729, 100486, 98854],\n",
       "  [92311, 3729, 100486, 98854],\n",
       "  [101578, 92406],\n",
       "  [92406],\n",
       "  [100486, 98854]],\n",
       " 'out_token_str': '銀行',\n",
       " 'latent_token_id_1': [[68498, 13968],\n",
       "  [6592, 13968],\n",
       "  [6565],\n",
       "  [5893],\n",
       "  [14503, 13968],\n",
       "  [28295, 13968],\n",
       "  [30321],\n",
       "  [24563]],\n",
       " 'latent_token_str_1': 'bank',\n",
       " 'latent_token_id_2': [[19177], [3001], [92311, 19177], [92311, 3001]],\n",
       " 'latent_token_str_2': '银行'}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# layer_idx = 32\n",
    "# num_beams = 8\n",
    "# n = 5\n",
    "# max_length = 1\n",
    "# prompt = dataset_gap[n]['prompt']\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# # prompt = dataset[4].enTja\n",
    "# results = custom_beam_search(model=model.model, tokenizer=tokenizer, device=device, prompt=prompt, num_beams=num_beams, max_length=max_length, layer_id=layer_idx)\n",
    "# # results = generate_intermediate_text_with_beam_search_hidden(model=model.model, tokenizer=tokenizer, device=device, layer_idx=39, prompt=prompt, num_beams=num_beams, max_length=max_length)\n",
    "# for result, id, probability in results:\n",
    "#     print(f\"Sequence: {result}, Ids: {id}, Probability: {probability:.4f}\")\n",
    "# dataset_gap[n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "67eb2ebf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 电话\n",
      "手机\n",
      " 手机\n",
      "电话\n"
     ]
    }
   ],
   "source": [
    "# for i in dataset_gap[0]['latent_token_id_2']:\n",
    "#     alternative_decoded_strings=tokenizer.decode(i)\n",
    "#     print(alternative_decoded_strings)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
